{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Example-keras-model\" data-toc-modified-id=\"Example-keras-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Example <code>keras</code> model</a></div><div class=\"lev2 toc-item\"><a href=\"#Simple-as-can-be\" data-toc-modified-id=\"Simple-as-can-be-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Simple as can be</a></div><div class=\"lev2 toc-item\"><a href=\"#Adding-dropout-for-the-learning-phase\" data-toc-modified-id=\"Adding-dropout-for-the-learning-phase-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Adding dropout for the learning phase</a></div><div class=\"lev2 toc-item\"><a href=\"#Multilayer-convolutional-neural-network\" data-toc-modified-id=\"Multilayer-convolutional-neural-network-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Multilayer convolutional neural network</a></div><div class=\"lev3 toc-item\"><a href=\"#Set-up\" data-toc-modified-id=\"Set-up-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Set-up</a></div><div class=\"lev3 toc-item\"><a href=\"#weight-initialization\" data-toc-modified-id=\"weight-initialization-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>weight initialization</a></div><div class=\"lev3 toc-item\"><a href=\"#Convolution-and-Pooling\" data-toc-modified-id=\"Convolution-and-Pooling-133\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Convolution and Pooling</a></div><div class=\"lev3 toc-item\"><a href=\"#First-convolutional-layer\" data-toc-modified-id=\"First-convolutional-layer-134\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>First convolutional layer</a></div><div class=\"lev3 toc-item\"><a href=\"#Second-convolutional-layer\" data-toc-modified-id=\"Second-convolutional-layer-135\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Second convolutional layer</a></div><div class=\"lev3 toc-item\"><a href=\"#Densely-connected-layer\" data-toc-modified-id=\"Densely-connected-layer-136\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Densely connected layer</a></div><div class=\"lev3 toc-item\"><a href=\"#Dropout-layer\" data-toc-modified-id=\"Dropout-layer-137\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Dropout layer</a></div><div class=\"lev3 toc-item\"><a href=\"#Readout-layer\" data-toc-modified-id=\"Readout-layer-138\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Readout layer</a></div><div class=\"lev3 toc-item\"><a href=\"#Setting-up-training\" data-toc-modified-id=\"Setting-up-training-139\"><span class=\"toc-item-num\">1.3.9&nbsp;&nbsp;</span>Setting up training</a></div><div class=\"lev3 toc-item\"><a href=\"#Running-training\" data-toc-modified-id=\"Running-training-1310\"><span class=\"toc-item-num\">1.3.10&nbsp;&nbsp;</span>Running training</a></div><div class=\"lev2 toc-item\"><a href=\"#Multilayer-CNN-with-keras\" data-toc-modified-id=\"Multilayer-CNN-with-keras-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Multilayer CNN with <code>keras</code></a></div><div class=\"lev3 toc-item\"><a href=\"#Setting-up-data-generation-from-only-1000-images-from-each-class\" data-toc-modified-id=\"Setting-up-data-generation-from-only-1000-images-from-each-class-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Setting up data generation from only 1000 images from each class</a></div><div class=\"lev3 toc-item\"><a href=\"#Setting-up-the-network\" data-toc-modified-id=\"Setting-up-the-network-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Setting up the network</a></div><div class=\"lev3 toc-item\"><a href=\"#Initializing-training\" data-toc-modified-id=\"Initializing-training-143\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Initializing training</a></div><div class=\"lev1 toc-item\"><a href=\"#Variance-Ratio-for-Active-Learning\" data-toc-modified-id=\"Variance-Ratio-for-Active-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Variance Ratio for Active Learning</a></div><div class=\"lev1 toc-item\"><a href=\"#Building-a-network-that-reads-in-images\" data-toc-modified-id=\"Building-a-network-that-reads-in-images-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Building a network that reads in images</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example `keras` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple as can be\n",
    "Note that this was taken from the [keras website](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html#calling-keras-layers-on-tensorflow-tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a variable called `img` to store the input images as flat vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=(None, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we construct the network using `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define the placeholder for the labels, and define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.objectives import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model  using a Tensorflow optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    for j in range(100):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0], \n",
    "                                  labels: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.metrics import categorical_accuracy as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8742\n"
     ]
    }
   ],
   "source": [
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print(sum(acc_value.eval(feed_dict={img:mnist_data.test.images,\n",
    "                                        labels:mnist_data.test.labels}))/len(mnist_data.test.images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding dropout for the learning phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy as accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate/set session: basically, tie the C++ `tensorflow` backend to `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set input and output variable placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct network with dropout; will take longer to train but should generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dense(128, activation='relu')(img)\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function using categorical cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model  using a Tensorflow optimizer, gradient descent. I cannot figure out if it's doing mini-batch SGD or regular ol' GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all variables in workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop â€” we are in the *learning phase* so we have to specify this in `feed_dict` so that `tf` and `keras` know how to use the `Dropout` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_error = []\n",
    "batch_number = []\n",
    "num_rounds = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_value = accuracy(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    for j in range(num_rounds):\n",
    "        batch = mnist_data.train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: batch[0], \n",
    "                                  labels: batch[1],\n",
    "                                  K.learning_phase():1})\n",
    "        if (j%50) == 1:\n",
    "            batch_number.append(j)\n",
    "            test_error.append(\n",
    "                sum(acc_value.eval(feed_dict={img:mnist_data.test.images,\n",
    "                                              labels: mnist_data.test.labels,\n",
    "                                              K.learning_phase():0})) /\n",
    "                mnist_data.test.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the final test error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    batch_number.append(num_rounds-1)\n",
    "    test_error.append(sum(acc_value.eval(feed_dict={img:mnist_data.test.images,\n",
    "                                                    labels:mnist_data.test.labels,\n",
    "                                                    K.learning_phase():0})) /\n",
    "                      mnist_data.test.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting accuracy after each recorded batch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFwCAYAAAAR/Lm5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12547b160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch_number, test_error);\n",
    "plt.xlabel('Number of batches')\n",
    "plt.ylabel('Categorical Accuracy (%)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial from [the tensorflow blog](https://www.tensorflow.org/get_started/mnist/pros#build_a_multilayer_convolutional_network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight initialization\n",
    "\n",
    "we want to initialize random weights (to avoid zero-gradients and for symmetry breaking) that lie away from zero (to avoid \"dead neurons\"). define two functions to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, \n",
    "                        strides=[1,1,1,1], \n",
    "                        padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], \n",
    "                          strides=[1,2,2,1], \n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, shape=[-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 1\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 0.92\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.98\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.96\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.94\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 1\n",
      "test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for j in range(2000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if (j % 100) == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict = {x: batch[0], \n",
    "                                                        y_:batch[1], \n",
    "                                                        keep_prob: 1.0})\n",
    "            print('step {:d}, training accuracy {:g}'.format(j, train_accuracy))\n",
    "        train_step.run(feed_dict={x:batch[0], y_:batch[1], keep_prob:.5})\n",
    "    # done for loop\n",
    "    print('test accuracy: {:g}'.format(accuracy.eval(feed_dict={x:batch[0], y_:batch[1], keep_prob:1.0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer CNN with `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this `keras` blog post](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data generation from only 1000 images from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=40, \n",
    "                             width_shift_range=.2, height_shift_range=.2, \n",
    "                             shear_range=.2, zoom_range=.2, \n",
    "                             fill_mode='nearest', horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), input_shape=(150, 150, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True,\n",
    "                                   shear_range=0.2,zoom_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'catsdogs/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'catsdogs/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 76s - loss: 0.7080 - acc: 0.5400 - val_loss: 0.6669 - val_acc: 0.6025\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 77s - loss: 0.6649 - acc: 0.6075 - val_loss: 0.6875 - val_acc: 0.5988\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 77s - loss: 0.6242 - acc: 0.6560 - val_loss: 0.6516 - val_acc: 0.6212\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 76s - loss: 0.6092 - acc: 0.6845 - val_loss: 0.6076 - val_acc: 0.6787\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 72s - loss: 0.5850 - acc: 0.7125 - val_loss: 0.6130 - val_acc: 0.6775\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 71s - loss: 0.5896 - acc: 0.7010 - val_loss: 0.5668 - val_acc: 0.6937\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 73s - loss: 0.5567 - acc: 0.7215 - val_loss: 0.5816 - val_acc: 0.6863\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 74s - loss: 0.5449 - acc: 0.7295 - val_loss: 0.5370 - val_acc: 0.7100\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 73s - loss: 0.5397 - acc: 0.7445 - val_loss: 0.5393 - val_acc: 0.7163\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 73s - loss: 0.5148 - acc: 0.7515 - val_loss: 0.4908 - val_acc: 0.7525\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 72s - loss: 0.5080 - acc: 0.7575 - val_loss: 0.5105 - val_acc: 0.7338\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 76s - loss: 0.4784 - acc: 0.7755 - val_loss: 0.4685 - val_acc: 0.7863\n",
      "Epoch 13/50\n",
      " 63/125 [==============>...............] - ETA: 31s - loss: 0.4801 - acc: 0.7659"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0af6dae6cb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         validation_steps=800 // batch_size)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first_try.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# always save your weights after training or during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1875\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1878\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2075\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/berkas/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2002 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Ratio for Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad, Adam\n",
    "from keras.utils import np_utils, generic_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a network that reads in images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
